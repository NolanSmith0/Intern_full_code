{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48993d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import joblib\n",
    "from time import sleep\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed5a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlGet(url, headers, max_try=3, sleep_time=5):\n",
    "    '''\n",
    "    self designed url get request using requests\n",
    "    '''\n",
    "    not_connected = True\n",
    "    tried = 0\n",
    "    while not_connected and tried < max_try:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                sleep(sleep_time)\n",
    "            else:\n",
    "                not_connected = False\n",
    "            tried += 1\n",
    "        except:\n",
    "            sleep(sleep_time)\n",
    "            tried += 1\n",
    "    if tried == max_try:\n",
    "        return None\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_source(source, prefix):\n",
    "    data1 = bs(source, 'html.parser')\n",
    "    parsed = []\n",
    "    for one in data1.select('div.content__list > div'):\n",
    "        tem = prefix.copy()\n",
    "        \n",
    "        try:\n",
    "            # 第一列数据\n",
    "            title = one.select_one('div > p > a').get_text().replace('\\n', '').strip()\n",
    "            # 小区名称\n",
    "            name = title.split('·')[-1].split(' ')[0]\n",
    "            # 整租/单租\n",
    "            renting = title.split('·')[0]\n",
    "\n",
    "            # 链接\n",
    "            url = urljoin('https://tj.lianjia.com/zufang/', one.select_one('div > p > a')['href'])\n",
    "\n",
    "            # 第二列数据\n",
    "            a = one.select_one('.content__list--item--des').get_text().split('\\n')\n",
    "            a = [i.replace('/', '').strip() for i in a if i.strip() not in ['', '/']]\n",
    "            if len(a) == 5:\n",
    "                mianji, chaoxiang, huxing, louceng = a[1:]\n",
    "                louceng = louceng.replace(' ', '')\n",
    "            else:\n",
    "                mianji, chaoxiang, huxing, louceng = a[1], '无', a[-1], '无' \n",
    "\n",
    "            # 第三列数据\n",
    "            tag = one.select_one('p:nth-child(3)').get_text().replace('\\n', ' ').strip()\n",
    "\n",
    "            # 价格\n",
    "            price = one.select_one('div > span').get_text().split(' ')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        tem.update({\n",
    "            '小区名称' : name, '链接' : url, '租赁方式' : renting, '面积' : mianji, '房屋朝向' : chaoxiang,\n",
    "            '户型' : huxing, '楼层' : louceng, '标签' : tag, '价格' : price[0], '价格单位' : price[1]\n",
    "        })\n",
    "        parsed.append(tem)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def get_count(response):\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    count = int(soup.select_one('.content__title--hl').get_text())\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_max_page(response):\n",
    "    return bs(response.text, 'html.parser').select_one('.content__pg')['data-totalpage']\n",
    "    \n",
    "    \n",
    "def split_series(series, n):\n",
    "\n",
    "    total_elements = len(series)\n",
    "    elements_per_partition, remainder = divmod(total_elements, n)\n",
    "\n",
    "    start = 0\n",
    "    partitions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        end = start + elements_per_partition + (1 if i < remainder else 0)\n",
    "        partition = series[start:end]\n",
    "        partitions.append(partition)\n",
    "        start = end\n",
    "\n",
    "    return partitions\n",
    "            \n",
    "       \n",
    "def collect_data(url_list, headers, prefix):\n",
    "    data = []\n",
    "    \n",
    "    def collect_single(url_list, header, prefix):\n",
    "        \n",
    "        nonlocal data\n",
    "        \n",
    "        for url in url_list:\n",
    "            response = urlGet(url, header)\n",
    "            if response == None:\n",
    "                print('get failed : ', url)\n",
    "                sleep(5)\n",
    "                continue\n",
    "            try:\n",
    "                parsed = parse_source(response.text, prefix)\n",
    "                data += parsed\n",
    "            except:\n",
    "                print('parsing failed : ', url)\n",
    "            sleep(3)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    headers_count = len(headers)\n",
    "    splitted_url_list = split_series(url_list, headers_count)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=headers_count) as executor:\n",
    "        futures = [executor.submit(collect_single, splitted_url_list[i], headers[i], prefix) for i in range(headers_count)]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d00daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tianjin():\n",
    "    global alldata\n",
    "    global failed\n",
    "    global headers\n",
    "    \n",
    "    tianjin_url = 'https://tj.lianjia.com/zufang/'\n",
    "    tianjin_response = urlGet(tianjin_url, headers[0])\n",
    "    if tianjin_response == None:\n",
    "        print('Failed to get 天津')\n",
    "        return None\n",
    "    tianjin_soup = bs(tianjin_response.text, 'html.parser')\n",
    "    districts = tianjin_soup.select_one('div.filter').find('ul', {'data-target' : 'area'}).find_all('a', {'rel' : None})\n",
    "    \n",
    "    for district in districts:\n",
    "        district_name = district.get_text()\n",
    "        print(f'\\t Scraping 天津 {district_name}')\n",
    "        district_url = urljoin(tianjin_url, district['href'])\n",
    "        district_response = urlGet(district_url, headers[1])\n",
    "        if district_response == None:\n",
    "            print(f'\\t {district_name} failed')\n",
    "            sleep(3)\n",
    "            continue\n",
    "        district_soup = bs(district_response.text, 'html.parser')\n",
    "        areas = district_soup.select_one('div.filter').find('ul', {'data-target' : 'area', 'class' : None}).find_all('a')[1:]\n",
    "        sleep(3)\n",
    "        \n",
    "        for area in areas:\n",
    "            area_name = area.get_text()\n",
    "            print(f'\\t\\t Scraping 天津 {district_name} {area_name}')\n",
    "            areat_url = urljoin(tianjin_url, area['href'])\n",
    "            area_response = urlGet(areat_url, headers[1])\n",
    "            if area_response == None:\n",
    "                print(f'\\t\\t {area_name} failed')\n",
    "                sleep(3)\n",
    "                continue\n",
    "            area_soup = bs(area_response.text, 'html.parser')\n",
    "            sleep(3)\n",
    "            if get_count(area_response) > 3000:\n",
    "                print(get_count(area_response))\n",
    "                print(f'\\t\\t {area_name} exceeding 3000')\n",
    "                failed.append(areat_url)\n",
    "                continue\n",
    "            # 获取爬取列表\n",
    "            try:\n",
    "                max_page = int(get_max_page(area_response))\n",
    "            except:\n",
    "                print('get max page faild : ', areat_url)\n",
    "                sleep(3)\n",
    "                continue\n",
    "            urls = [areat_url+f'pg{i+1}/' for i in range(max_page)]\n",
    "            # 爬取数据\n",
    "            prefix = {'城市' : '天津', '地区' : district_name, '区域' : area_name}\n",
    "            alldata += collect_data(urls, headers, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722cd459",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = joblib.load('./headers')\n",
    "alldata, failed = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e8cce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Scraping 天津 和平\n",
      "\t\t Scraping 天津 和平 南营门街\n",
      "\t\t Scraping 天津 和平 南市\n",
      "\t\t Scraping 天津 和平 劝业场\n",
      "\t\t Scraping 天津 和平 体育馆街\n",
      "\t\t Scraping 天津 和平 小白楼\n",
      "\t\t Scraping 天津 和平 新兴街\n",
      "\t Scraping 天津 南开\n",
      "\t\t Scraping 天津 南开 长虹街\n",
      "\t\t Scraping 天津 南开 广开街\n",
      "\t\t Scraping 天津 南开 鼓楼街\n",
      "\t\t Scraping 天津 南开 华苑\n",
      "\t\t Scraping 天津 南开 嘉陵道街\n",
      "\t\t Scraping 天津 南开 水上公园街\n",
      "\t\t Scraping 天津 南开 体育中心街\n",
      "get failed :  https://tj.lianjia.com/zufang/tiyuzhongxinjie/pg12/\n",
      "\t\t Scraping 天津 南开 王顶堤\n",
      "\t\t Scraping 天津 南开 万兴街\n",
      "\t\t Scraping 天津 南开 兴南街\n",
      "\t\t Scraping 天津 南开 学府街\n",
      "\t\t Scraping 天津 南开 向阳路\n",
      "\t Scraping 天津 河西\n",
      "\t\t Scraping 天津 河西 陈塘庄\n",
      "\t\t Scraping 天津 河西 大营门\n",
      "\t\t Scraping 天津 河西 东海街\n",
      "\t\t Scraping 天津 河西 挂甲寺\n",
      "\t\t Scraping 天津 河西 尖山\n",
      "\t\t Scraping 天津 河西 柳林街\n",
      "\t\t Scraping 天津 河西 马场街\n",
      "\t\t Scraping 天津 河西 梅江\n",
      "\t\t Scraping 天津 河西 天塔街\n",
      "\t\t Scraping 天津 河西 桃园街\n",
      "\t\t Scraping 天津 河西 新梅江\n",
      "\t\t Scraping 天津 河西 下瓦房\n",
      "\t\t Scraping 天津 河西 越秀路\n",
      "\t\t Scraping 天津 河西 友谊路\n",
      "\t Scraping 天津 河北\n",
      "\t\t Scraping 天津 河北 光复道\n",
      "\t\t Scraping 天津 河北 鸿顺里街\n",
      "\t\t Scraping 天津 河北 江都路\n",
      "\t\t Scraping 天津 河北 靖江路\n",
      "\t\t Scraping 天津 河北 建昌道\n",
      "\t\t Scraping 天津 河北 宁园\n",
      "\t\t Scraping 天津 河北 铁东路\n",
      "\t\t Scraping 天津 河北 王串场\n",
      "\t\t Scraping 天津 河北 望海楼\n",
      "\t\t Scraping 天津 河北 新开河\n",
      "\t\t Scraping 天津 河北 月牙河\n",
      "\t Scraping 天津 河东\n",
      "\t\t Scraping 天津 河东 春华街\n",
      "\t\t Scraping 天津 河东 常州道\n",
      "\t\t Scraping 天津 河东 大王庄\n",
      "\t\t Scraping 天津 河东 大直沽\n",
      "\t\t Scraping 天津 河东 二号桥街\n",
      "\t\t Scraping 天津 河东 富民路\n",
      "\t\t Scraping 天津 河东 鲁山道\n",
      "get max page faild :  https://tj.lianjia.com/zufang/lushandao/\n",
      "\t\t Scraping 天津 河东 上杭路\n",
      "\t\t Scraping 天津 河东 唐家口\n",
      "\t\t Scraping 天津 河东 太阳城\n",
      "\t\t Scraping 天津 河东 万新村\n",
      "\t\t Scraping 天津 河东 向阳楼\n",
      "\t\t Scraping 天津 河东 真理道\n",
      "get max page faild :  https://tj.lianjia.com/zufang/zhenlidao/\n",
      "\t\t Scraping 天津 河东 中山门街\n",
      "\t Scraping 天津 红桥\n",
      "\t\t Scraping 天津 红桥 大胡同\n",
      "\t\t Scraping 天津 红桥 丁字沽街\n",
      "\t\t Scraping 天津 红桥 芥园道\n",
      "\t\t Scraping 天津 红桥 铃铛阁\n",
      "\t\t Scraping 天津 红桥 三条石\n",
      "\t\t Scraping 天津 红桥 双环邨\n",
      "\t\t Scraping 天津 红桥 邵公庄\n",
      "\t\t Scraping 天津 红桥 天穆镇\n",
      "\t\t Scraping 天津 红桥 咸阳北路\n",
      "\t\t Scraping 天津 红桥 西沽街\n",
      "\t\t Scraping 天津 红桥 西于庄\n",
      "\t Scraping 天津 西青\n",
      "\t\t Scraping 天津 西青 滨海高新区\n",
      "\t\t Scraping 天津 西青 大寺\n",
      "\t\t Scraping 天津 西青 侯台\n",
      "\t\t Scraping 天津 西青 精武镇\n",
      "\t\t Scraping 天津 西青 津门湖\n",
      "\t\t Scraping 天津 西青 李七庄\n",
      "\t\t Scraping 天津 西青 西营门\n",
      "\t\t Scraping 天津 西青 辛口镇\n",
      "\t\t Scraping 天津 西青 杨柳青\n",
      "\t\t Scraping 天津 西青 张家窝\n",
      "\t\t Scraping 天津 西青 中北镇\n",
      "\t Scraping 天津 北辰\n",
      "\t\t Scraping 天津 北辰 北仓镇\n",
      "\t\t Scraping 天津 北辰 果园新村\n",
      "\t\t Scraping 天津 北辰 集贤里\n",
      "\t\t Scraping 天津 北辰 瑞景居住区\n",
      "\t\t Scraping 天津 北辰 双口镇\n",
      "\t\t Scraping 天津 北辰 双环邨\n",
      "\t\t Scraping 天津 北辰 双街\n",
      "\t\t Scraping 天津 北辰 天穆镇\n",
      "\t\t Scraping 天津 北辰 小淀镇\n",
      "\t\t Scraping 天津 北辰 西堤头\n",
      "\t\t Scraping 天津 北辰 宜兴埠\n",
      "get failed :  https://tj.lianjia.com/zufang/yixingbu/pg12/\n",
      "get failed :  https://tj.lianjia.com/zufang/yixingbu/pg18/\n",
      "get failed :  https://tj.lianjia.com/zufang/yixingbu/pg2/\n",
      "\t Scraping 天津 东丽\n",
      "\t\t Scraping 天津 东丽 东丽湖\n",
      "\t\t Scraping 天津 东丽 钢管公司\n",
      "\t\t Scraping 天津 东丽 华明镇\n",
      "\t\t Scraping 天津 东丽 军粮城\n",
      "\t\t Scraping 天津 东丽 金钟街\n",
      "\t\t Scraping 天津 东丽 空港经济区\n",
      "\t\t Scraping 天津 东丽 万新街\n",
      "\t\t Scraping 天津 东丽 新立街\n",
      "\t\t Scraping 天津 东丽 张贵庄\n",
      "\t Scraping 天津 津南\n",
      "\t\t Scraping 天津 津南 八里台\n",
      "\t\t Scraping 天津 津南 北闸口\n",
      "\t\t Scraping 天津 津南 葛沽镇\n",
      "\t\t Scraping 天津 津南 海河教育园区\n",
      "\t\t Scraping 天津 津南 双林\n",
      "\t\t Scraping 天津 津南 双港\n",
      "\t\t Scraping 天津 津南 双桥河\n",
      "\t\t Scraping 天津 津南 咸水沽\n",
      "\t\t Scraping 天津 津南 小站镇\n",
      "\t\t Scraping 天津 津南 辛庄\n",
      "\t Scraping 天津 武清\n",
      "\t\t Scraping 天津 武清 保利金街\n",
      "\t\t Scraping 天津 武清 大王古镇\n",
      "\t\t Scraping 天津 武清 东蒲洼街\n",
      "\t\t Scraping 天津 武清 佛罗伦萨\n",
      "\t\t Scraping 天津 武清 高村\n",
      "\t\t Scraping 天津 武清 河西务\n",
      "\t\t Scraping 天津 武清 黄庄\n",
      "\t\t Scraping 天津 武清 静湖\n",
      "\t\t Scraping 天津 武清 南湖\n",
      "\t\t Scraping 天津 武清 泗村\n",
      "\t\t Scraping 天津 武清 体育中心\n",
      "\t\t Scraping 天津 武清 武清其它\n",
      "get failed :  https://tj.lianjia.com/zufang/wuqingqita/pg3/\n",
      "get failed :  https://tj.lianjia.com/zufang/wuqingqita/pg7/\n",
      "get failed :  https://tj.lianjia.com/zufang/wuqingqita/pg21/\n",
      "get failed :  https://tj.lianjia.com/zufang/wuqingqita/pg8/\n",
      "\t\t Scraping 天津 武清 徐官屯\n",
      "get max page faild :  https://tj.lianjia.com/zufang/xuguantun/\n",
      "\t\t Scraping 天津 武清 下朱庄\n",
      "get failed :  https://tj.lianjia.com/zufang/xiazhuzhuang/pg14/\n",
      "\t\t Scraping 天津 武清 新湾\n",
      "\t\t Scraping 天津 武清 杨村\n",
      "\t\t Scraping 天津 武清 中信广场\n",
      "\t Scraping 天津 滨海新区\n",
      "\t\t Scraping 天津 滨海新区 滨海其它\n",
      "\t\t Scraping 天津 滨海新区 渤海石油街\n",
      "\t\t Scraping 天津 滨海新区 北塘街\n",
      "\t\t Scraping 天津 滨海新区 茶淀街\n",
      "\t\t Scraping 天津 滨海新区 东疆港\n",
      "\t\t Scraping 天津 滨海新区 大港街\n",
      "\t\t Scraping 天津 滨海新区 大沽街\n",
      "\t\t Scraping 天津 滨海新区 古林街\n",
      "\t\t Scraping 天津 滨海新区 胡家园\n",
      "\t\t Scraping 天津 滨海新区 杭州道\n",
      "\t\t Scraping 天津 滨海新区 汉沽街\n",
      "\t\t Scraping 天津 滨海新区 海滨街\n",
      "\t\t Scraping 天津 滨海新区 开发区\n",
      "\t\t Scraping 天津 滨海新区 三槐路街\n",
      "get max page faild :  https://tj.lianjia.com/zufang/sanhuailujie/\n",
      "\t\t Scraping 天津 滨海新区 天津港\n",
      "\t\t Scraping 天津 滨海新区 塘沽街\n",
      "\t\t Scraping 天津 滨海新区 太平镇\n",
      "\t\t Scraping 天津 滨海新区 新村街\n",
      "\t\t Scraping 天津 滨海新区 新河街\n",
      "\t\t Scraping 天津 滨海新区 新城镇\n",
      "\t\t Scraping 天津 滨海新区 新港街\n",
      "\t\t Scraping 天津 滨海新区 小王庄镇\n",
      "get max page faild :  https://tj.lianjia.com/zufang/xiaowangzhuangzhen/\n",
      "\t\t Scraping 天津 滨海新区 向阳街\n",
      "get max page faild :  https://tj.lianjia.com/zufang/xiangyangjie/\n",
      "\t\t Scraping 天津 滨海新区 新北街\n",
      "\t\t Scraping 天津 滨海新区 杨家泊镇\n",
      "\t\t Scraping 天津 滨海新区 寨上街\n",
      "\t\t Scraping 天津 滨海新区 中塘镇\n",
      "\t\t Scraping 天津 滨海新区 中新生态城\n",
      "\t Scraping 天津 宝坻\n",
      "\t\t Scraping 天津 宝坻 宝坻其他东\n",
      "\t\t Scraping 天津 宝坻 北城路\n",
      "\t\t Scraping 天津 宝坻 宝坻其他西\n",
      "\t\t Scraping 天津 宝坻 潮阳大道\n",
      "\t\t Scraping 天津 宝坻 翡翠湾\n",
      "\t\t Scraping 天津 宝坻 海滨\n",
      "\t\t Scraping 天津 宝坻 火车站\n",
      "\t\t Scraping 天津 宝坻 建设路\n",
      "\t\t Scraping 天津 宝坻 京津新城\n",
      "\t\t Scraping 天津 宝坻 马家店\n",
      "\t\t Scraping 天津 宝坻 南关大街\n",
      "\t\t Scraping 天津 宝坻 南三路\n",
      "\t\t Scraping 天津 宝坻 怡购\n",
      "\t Scraping 天津 蓟州\n",
      "\t\t Scraping 天津 蓟州 津围线东\n",
      "\t\t Scraping 天津 蓟州 蓟州城区\n",
      "\t\t Scraping 天津 蓟州 津围线西\n",
      "\t\t Scraping 天津 蓟州 盘山风景区\n",
      "\t\t Scraping 天津 蓟州 人民公园\n",
      "\t\t Scraping 天津 蓟州 于桥水库\n",
      "\t Scraping 天津 静海\n",
      "\t\t Scraping 天津 静海 静海其他\n",
      "\t\t Scraping 天津 静海 静海\n",
      "\t\t Scraping 天津 静海 团泊西\n",
      "\t\t Scraping 天津 静海 团泊东\n"
     ]
    }
   ],
   "source": [
    "tianjin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c58d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(alldata).sort_values(['地区', '区域']).to_csv('./天津市租房信息.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
